{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project #1: Baseball Analytics\n",
    "\n",
    "The overall purpose of this mini-project is to predicting MLB wins per season by modeling data to KMeans clustering model and linear regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Analysis/Modeling\n",
    "\n",
    "In this part of the project, you are going to conduct actual analysis on the data your just processed in Part 1 & 2. The tasks in the part include:\n",
    "- K-means Clustering: pre-modeling part that provides insights toward the data;\n",
    "- Linear Regression: predict Wins (continuous) using trained linear regression model;\n",
    "- Logistic Regression: predict Win_bins (categorical) using trained logistic regression model __on your own__.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     G    R    AB     H   2B  3B   HR   BB      SO   SB    ...     \\\n",
      "0  162  744  5424  1331  218  22  189  681  1068.0   37    ...      \n",
      "1  162  718  5499  1377  232  35  137  602   917.0   46    ...      \n",
      "2  161  597  5506  1378  208  38   95  448   916.0   43    ...      \n",
      "3  162  544  5362  1297  186  27  102  472   920.0   49    ...      \n",
      "4  162  527  5354  1279  200  36   92  443   973.0  107    ...      \n",
      "\n",
      "   decade_1950  decade_1960  decade_1970  decade_1980  decade_1990  \\\n",
      "0            0            1            0            0            0   \n",
      "1            0            1            0            0            0   \n",
      "2            0            1            0            0            0   \n",
      "3            0            1            0            0            0   \n",
      "4            0            1            0            0            0   \n",
      "\n",
      "   decade_2000  decade_2010  R_per_game  RA_per_game       rpg  \n",
      "0            0            0    4.592593     4.839506  4.525175  \n",
      "1            0            0    4.432099     4.358025  4.460518  \n",
      "2            0            0    3.708075     4.099379  3.946881  \n",
      "3            0            0    3.358025     3.401235  4.035670  \n",
      "4            0            0    3.253086     3.512346  3.988293  \n",
      "\n",
      "[5 rows x 38 columns]\n",
      "   wins\n",
      "0    70\n",
      "1    86\n",
      "2    70\n",
      "3    82\n",
      "4    75\n",
      "   win_bins\n",
      "0         3\n",
      "1         3\n",
      "2         3\n",
      "3         3\n",
      "4         3\n"
     ]
    }
   ],
   "source": [
    "# read-in required data\n",
    "# features for analysis\n",
    "data_features = pd.read_csv('../ba545-week4-ClarkeDee/baseball_analytics_features.csv', header=0, index_col=0)\n",
    "\n",
    "# continuous target `wins`\n",
    "wins = pd.read_csv('../ba545-week4-ClarkeDee/baseball_analytics_wins.csv',  index_col=0, names = ['wins'])\n",
    "\n",
    "# categorical target `Win_bins`\n",
    "win_bins = pd.read_csv('../ba545-week4-ClarkeDee/baseball_analytics_target.csv',  index_col=0, names = ['win_bins'])\n",
    "\n",
    "# display if data are read correctly\n",
    "print(data_features.head())\n",
    "print(wins.head())\n",
    "print(win_bins.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means Clustering\n",
    "\n",
    "K-means clustering, as a basic clustering technique, can capture internal relationship(s) between your data points. Sometimes we use (k-means) clustering as a pre-modeling step for supervised learning: essentially, we can use k-means clsutering to capture the internal relationship of the features, and then capture the relationship in an additional feature that being used as an input to a classification/regression model.\n",
    "\n",
    "One key step in k-means clustering is to determine the value of `k` - how many clusters? If we want to use the clustering results as an additional (categorical) feature, we should not have a higher value of `k`. Also, increasing value of `k` may increase the erroneous relationship being captured. The k-means model is provided in `sklearn.clustering`.\n",
    "\n",
    "In this tutorial, we use **Grid Search** to find the best value of `k`. To conduct Grid Search, you need a range of `k` and a metric that measures the performance under each value of `k`. In this context, we select the metric as the [**silhouette score**](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html) (`s_score`), which is provided in `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette score is a visualized way of measuring the performance of clustering. Thus, we need to import `matplotlib` to visualize the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and initialize matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '<bound method DataFrame.median of       yearID teamID    G   W    R    AB     H   2B  3B   HR  ...    SHO  SV  \\\\\\n0       1961    LAA  162  70  744  5424  1331  218  22  189  ...      5  34   \\n1       1962    LAA  162  86  718  5499  1377  232  35  137  ...     15  47   \\n2       1963    LAA  161  70  597  5506  1378  208  38   95  ...     13  31   \\n3       1964    LAA  162  82  544  5362  1297  186  27  102  ...     28  41   \\n4       1965    CAL  162  75  527  5354  1279  200  36   92  ...     14  33   \\n5       1966    CAL  162  80  604  5360  1244  179  54  122  ...     12  40   \\n6       1967    CAL  161  84  567  5307  1265  170  37  114  ...     14  46   \\n7       1968    CAL  162  67  498  5331  1209  170  33   83  ...     11  31   \\n8       1969    CAL  163  71  528  5316  1221  151  29   88  ...      9  39   \\n9       1970    CAL  162  86  631  5532  1391  197  40  114  ...     10  49   \\n10      1971    CAL  162  76  511  5495  1271  213  18   96  ...     11  32   \\n11      1972    CAL  155  75  454  5165  1249  171  26   78  ...     18  16   \\n12      1973    CAL  162  79  629  5505  1395  183  29   93  ...     13  19   \\n13      1974    CAL  163  68  618  5401  1372  203  31   95  ...     13  12   \\n14      1975    CAL  161  72  628  5377  1324  195  41   55  ...     19  16   \\n15      1976    CAL  162  76  550  5385  1265  210  23   63  ...     15  17   \\n16      1977    CAL  162  74  675  5410  1380  233  40  131  ...     13  26   \\n17      1978    CAL  162  87  691  5472  1417  226  28  108  ...     13  33   \\n18      1979    CAL  162  88  866  5550  1563  242  43  164  ...      9  33   \\n19      1980    CAL  160  65  698  5443  1442  236  32  106  ...      6  30   \\n20      1982    CAL  162  93  814  5532  1518  268  26  186  ...     10  27   \\n21      1983    CAL  162  70  722  5640  1467  241  22  154  ...      7  23   \\n22      1984    CAL  162  81  696  5470  1363  211  30  150  ...     12  26   \\n23      1985    CAL  162  90  732  5442  1364  215  31  153  ...      8  41   \\n24      1986    CAL  162  92  786  5433  1387  236  36  167  ...     12  40   \\n25      1987    CAL  162  75  770  5570  1406  257  26  172  ...      7  36   \\n26      1988    CAL  162  75  714  5582  1458  258  31  124  ...      9  33   \\n27      1989    CAL  162  91  669  5545  1422  208  37  145  ...     20  38   \\n28      1990    CAL  162  80  690  5570  1448  237  27  147  ...     13  42   \\n29      1991    CAL  162  81  653  5470  1396  245  29  115  ...     10  50   \\n...      ...    ...  ...  ..  ...   ...   ...  ...  ..  ...  ...    ...  ..   \\n2257    1985    MON  161  84  633  5429  1342  242  49  118  ...     13  53   \\n2258    1986    MON  161  78  637  5508  1401  255  50  110  ...      9  50   \\n2259    1987    MON  162  91  741  5527  1467  310  39  120  ...      8  50   \\n2260    1988    MON  163  81  628  5573  1400  260  48  107  ...     12  43   \\n2261    1989    MON  162  81  632  5482  1353  267  30  100  ...     13  35   \\n2262    1990    MON  162  85  662  5453  1363  227  43  114  ...     11  50   \\n2263    1991    MON  161  71  579  5412  1329  236  42   95  ...     14  39   \\n2264    1992    MON  162  87  648  5477  1381  263  37  102  ...     14  49   \\n2265    1993    MON  163  94  732  5493  1410  270  36  122  ...      7  61   \\n2266    1996    MON  162  88  741  5505  1441  297  27  148  ...      7  43   \\n2267    1997    MON  162  78  691  5526  1423  339  34  172  ...     14  37   \\n2268    1998    MON  162  65  644  5418  1348  280  32  147  ...      5  39   \\n2269    1999    MON  162  68  718  5559  1473  320  47  163  ...      4  44   \\n2270    2000    MON  162  67  738  5535  1475  310  35  178  ...      7  39   \\n2271    2001    MON  162  68  670  5379  1361  320  28  131  ...     11  28   \\n2272    2002    MON  162  83  735  5479  1432  300  36  162  ...      3  39   \\n2273    2003    MON  162  83  711  5437  1404  294  25  144  ...     10  42   \\n2274    2004    MON  162  67  635  5474  1361  276  27  151  ...     11  31   \\n2275    2005    WAS  162  81  639  5426  1367  311  32  117  ...      9  51   \\n2276    2006    WAS  162  71  746  5495  1437  322  22  164  ...      3  32   \\n2277    2007    WAS  162  73  673  5520  1415  309  31  123  ...      6  46   \\n2278    2008    WAS  161  59  641  5491  1376  269  26  117  ...      8  28   \\n2279    2009    WAS  162  59  710  5493  1416  271  38  156  ...      3  33   \\n2280    2010    WAS  162  69  655  5418  1355  250  31  149  ...      5  37   \\n2281    2011    WAS  161  80  624  5441  1319  257  22  154  ...     10  49   \\n2282    2012    WAS  162  98  731  5615  1468  301  25  194  ...      9  51   \\n2283    2013    WAS  162  86  656  5436  1365  259  27  161  ...     13  47   \\n2284    2014    WAS  162  96  686  5542  1403  265  27  152  ...     19  45   \\n2285    2015    WAS  162  83  703  5428  1363  265  13  177  ...     13  41   \\n2286    2016    WAS  162  95  763  5490  1403  268  29  203  ...     12  46   \\n\\n      IPouts    HA  HRA  BBA   SOA    E     DP     FP  \\n0       4314  1391  180  713   973  192  154.0  0.969  \\n1       4398  1412  118  616   858  175  153.0  0.972  \\n2       4365  1317  120  578   889  163  155.0  0.974  \\n3       4350  1273  100  530   965  138  168.0  0.978  \\n4       4323  1259   91  563   847  123  149.0  0.981  \\n5       4371  1364  136  511   836  136  186.0  0.979  \\n6       4290  1246  118  525   892  111  135.0  0.982  \\n7       4311  1234  131  519   869  140  156.0  0.977  \\n8       4314  1294  126  517   885  135  164.0  0.978  \\n9       4386  1280  154  559   922  127  169.0  0.980  \\n10      4443  1246  101  607   904  131  159.0  0.980  \\n11      4131  1109   90  620  1000  114  135.0  0.981  \\n12      4368  1351  104  614  1010  156  153.0  0.975  \\n13      4317  1339  101  649   986  147  150.0  0.976  \\n14      4359  1386  123  613   975  184  164.0  0.971  \\n15      4431  1323   95  553   992  150  139.0  0.977  \\n16      4311  1383  136  572   965  147  137.0  0.976  \\n17      4365  1382  125  599   892  136  136.0  0.978  \\n18      4308  1463  131  573   820  135  172.0  0.978  \\n19      4284  1548  141  529   725  134  144.0  0.977  \\n20      4392  1436  124  482   728  106  171.0  0.983  \\n21      4422  1636  130  496   668  154  190.0  0.977  \\n22      4374  1526  143  474   754  128  170.0  0.980  \\n23      4371  1453  171  514   767  112  202.0  0.982  \\n24      4368  1356  153  478   955  107  156.0  0.983  \\n25      4371  1481  212  504   941  117  162.0  0.981  \\n26      4365  1503  135  568   817  135  175.0  0.979  \\n27      4362  1384  113  465   897   96  173.0  0.985  \\n28      4362  1482  106  544   944  140  186.0  0.977  \\n29      4323  1351  141  543   990  102  156.0  0.984  \\n...      ...   ...  ...  ...   ...  ...    ...    ...  \\n2257    4371  1346   99  509   870  121  152.0  0.981  \\n2258    4398  1350  119  566  1051  133  132.0  0.979  \\n2259    4350  1428  145  446  1012  147  122.0  0.976  \\n2260    4446  1310  122  476   923  142  145.0  0.978  \\n2261    4404  1344  120  519  1059  136  126.0  0.979  \\n2262    4419  1349  127  510   991  110  134.0  0.982  \\n2263    4320  1304  111  584   909  131  128.0  0.979  \\n2264    4404  1296   92  525  1014  124  113.0  0.980  \\n2265    4368  1369  119  521   934  159  144.0  0.975  \\n2266    4324  1353  152  482  1206  126  121.0  0.980  \\n2267    4341  1365  149  557  1138  131  150.0  0.979  \\n2268    4281  1448  156  533  1017  155  127.0  0.975  \\n2269    4302  1505  152  572  1043  160  126.0  0.974  \\n2270    4274  1575  181  579  1011  132  151.0  0.978  \\n2271    4294  1509  190  525  1103  108  139.0  0.982  \\n2272    4359  1475  165  508  1088  139  160.0  0.978  \\n2273    4313  1467  181  463  1028  102  152.0  0.983  \\n2274    4341  1477  191  582  1032   99  172.0  0.984  \\n2275    4374  1456  140  539   997   92  156.0  0.985  \\n2276    4309  1535  193  584   960  131  123.0  0.978  \\n2277    4340  1502  187  580   931  109  153.0  0.982  \\n2278    4302  1496  190  588  1063  123  143.0  0.980  \\n2279    4273  1533  173  629   911  143  155.0  0.977  \\n2280    4305  1469  151  512  1068  127  148.0  0.979  \\n2281    4348  1403  129  477  1049  104  145.0  0.983  \\n2282    4405  1296  129  497  1325   94  134.0  0.985  \\n2283    4337  1367  142  405  1236  107  146.0  0.982  \\n2284    4412  1351  110  352  1288  100  139.0  0.984  \\n2285    4304  1366  145  364  1342   90  125.0  0.985  \\n2286    4379  1272  155  468  1476   73  142.0  0.988  \\n\\n[2287 rows x 29 columns]>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-54139ba862b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# any clustering model needs a distance metric, in this case, `distance` is the distance between\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# any pair of data points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m# clustering models will generate `labels` - if you want to create the additional feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# as discussed above, you will use `labels` as its values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0;31m# XXX This skips _check_test_data, which may change the dtype;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;31m# we should refactor the input validation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36m_check_fit_data\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    856\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_fit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;34m\"\"\"Verify that the number of samples given is larger than k\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             raise ValueError(\"n_samples=%d should be >= n_clusters=%d\" % (\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '<bound method DataFrame.median of       yearID teamID    G   W    R    AB     H   2B  3B   HR  ...    SHO  SV  \\\\\\n0       1961    LAA  162  70  744  5424  1331  218  22  189  ...      5  34   \\n1       1962    LAA  162  86  718  5499  1377  232  35  137  ...     15  47   \\n2       1963    LAA  161  70  597  5506  1378  208  38   95  ...     13  31   \\n3       1964    LAA  162  82  544  5362  1297  186  27  102  ...     28  41   \\n4       1965    CAL  162  75  527  5354  1279  200  36   92  ...     14  33   \\n5       1966    CAL  162  80  604  5360  1244  179  54  122  ...     12  40   \\n6       1967    CAL  161  84  567  5307  1265  170  37  114  ...     14  46   \\n7       1968    CAL  162  67  498  5331  1209  170  33   83  ...     11  31   \\n8       1969    CAL  163  71  528  5316  1221  151  29   88  ...      9  39   \\n9       1970    CAL  162  86  631  5532  1391  197  40  114  ...     10  49   \\n10      1971    CAL  162  76  511  5495  1271  213  18   96  ...     11  32   \\n11      1972    CAL  155  75  454  5165  1249  171  26   78  ...     18  16   \\n12      1973    CAL  162  79  629  5505  1395  183  29   93  ...     13  19   \\n13      1974    CAL  163  68  618  5401  1372  203  31   95  ...     13  12   \\n14      1975    CAL  161  72  628  5377  1324  195  41   55  ...     19  16   \\n15      1976    CAL  162  76  550  5385  1265  210  23   63  ...     15  17   \\n16      1977    CAL  162  74  675  5410  1380  233  40  131  ...     13  26   \\n17      1978    CAL  162  87  691  5472  1417  226  28  108  ...     13  33   \\n18      1979    CAL  162  88  866  5550  1563  242  43  164  ...      9  33   \\n19      1980    CAL  160  65  698  5443  1442  236  32  106  ...      6  30   \\n20      1982    CAL  162  93  814  5532  1518  268  26  186  ...     10  27   \\n21      1983    CAL  162  70  722  5640  1467  241  22  154  ...      7  23   \\n22      1984    CAL  162  81  696  5470  1363  211  30  150  ...     12  26   \\n23      1985    CAL  162  90  732  5442  1364  215  31  153  ...      8  41   \\n24      1986    CAL  162  92  786  5433  1387  236  36  167  ...     12  40   \\n25      1987    CAL  162  75  770  5570  1406  257  26  172  ...      7  36   \\n26      1988    CAL  162  75  714  5582  1458  258  31  124  ...      9  33   \\n27      1989    CAL  162  91  669  5545  1422  208  37  145  ...     20  38   \\n28      1990    CAL  162  80  690  5570  1448  237  27  147  ...     13  42   \\n29      1991    CAL  162  81  653  5470  1396  245  29  115  ...     10  50   \\n...      ...    ...  ...  ..  ...   ...   ...  ...  ..  ...  ...    ...  ..   \\n2257    1985    MON  161  84  633  5429  1342  242  49  118  ...     13  53   \\n2258    1986    MON  161  78  637  5508  1401  255  50  110  ...      9  50   \\n2259    1987    MON  162  91  741  5527  1467  310  39  120  ...      8  50   \\n2260    1988    MON  163  81  628  5573  1400  260  48  107  ...     12  43   \\n2261    1989    MON  162  81  632  5482  1353  267  30  100  ...     13  35   \\n2262    1990    MON  162  85  662  5453  1363  227  43  114  ...     11  50   \\n2263    1991    MON  161  71  579  5412  1329  236  42   95  ...     14  39   \\n2264    1992    MON  162  87  648  5477  1381  263  37  102  ...     14  49   \\n2265    1993    MON  163  94  732  5493  1410  270  36  122  ...      7  61   \\n2266    1996    MON  162  88  741  5505  1441  297  27  148  ...      7  43   \\n2267    1997    MON  162  78  691  5526  1423  339  34  172  ...     14  37   \\n2268    1998    MON  162  65  644  5418  1348  280  32  147  ...      5  39   \\n2269    1999    MON  162  68  718  5559  1473  320  47  163  ...      4  44   \\n2270    2000    MON  162  67  738  5535  1475  310  35  178  ...      7  39   \\n2271    2001    MON  162  68  670  5379  1361  320  28  131  ...     11  28   \\n2272    2002    MON  162  83  735  5479  1432  300  36  162  ...      3  39   \\n2273    2003    MON  162  83  711  5437  1404  294  25  144  ...     10  42   \\n2274    2004    MON  162  67  635  5474  1361  276  27  151  ...     11  31   \\n2275    2005    WAS  162  81  639  5426  1367  311  32  117  ...      9  51   \\n2276    2006    WAS  162  71  746  5495  1437  322  22  164  ...      3  32   \\n2277    2007    WAS  162  73  673  5520  1415  309  31  123  ...      6  46   \\n2278    2008    WAS  161  59  641  5491  1376  269  26  117  ...      8  28   \\n2279    2009    WAS  162  59  710  5493  1416  271  38  156  ...      3  33   \\n2280    2010    WAS  162  69  655  5418  1355  250  31  149  ...      5  37   \\n2281    2011    WAS  161  80  624  5441  1319  257  22  154  ...     10  49   \\n2282    2012    WAS  162  98  731  5615  1468  301  25  194  ...      9  51   \\n2283    2013    WAS  162  86  656  5436  1365  259  27  161  ...     13  47   \\n2284    2014    WAS  162  96  686  5542  1403  265  27  152  ...     19  45   \\n2285    2015    WAS  162  83  703  5428  1363  265  13  177  ...     13  41   \\n2286    2016    WAS  162  95  763  5490  1403  268  29  203  ...     12  46   \\n\\n      IPouts    HA  HRA  BBA   SOA    E     DP     FP  \\n0       4314  1391  180  713   973  192  154.0  0.969  \\n1       4398  1412  118  616   858  175  153.0  0.972  \\n2       4365  1317  120  578   889  163  155.0  0.974  \\n3       4350  1273  100  530   965  138  168.0  0.978  \\n4       4323  1259   91  563   847  123  149.0  0.981  \\n5       4371  1364  136  511   836  136  186.0  0.979  \\n6       4290  1246  118  525   892  111  135.0  0.982  \\n7       4311  1234  131  519   869  140  156.0  0.977  \\n8       4314  1294  126  517   885  135  164.0  0.978  \\n9       4386  1280  154  559   922  127  169.0  0.980  \\n10      4443  1246  101  607   904  131  159.0  0.980  \\n11      4131  1109   90  620  1000  114  135.0  0.981  \\n12      4368  1351  104  614  1010  156  153.0  0.975  \\n13      4317  1339  101  649   986  147  150.0  0.976  \\n14      4359  1386  123  613   975  184  164.0  0.971  \\n15      4431  1323   95  553   992  150  139.0  0.977  \\n16      4311  1383  136  572   965  147  137.0  0.976  \\n17      4365  1382  125  599   892  136  136.0  0.978  \\n18      4308  1463  131  573   820  135  172.0  0.978  \\n19      4284  1548  141  529   725  134  144.0  0.977  \\n20      4392  1436  124  482   728  106  171.0  0.983  \\n21      4422  1636  130  496   668  154  190.0  0.977  \\n22      4374  1526  143  474   754  128  170.0  0.980  \\n23      4371  1453  171  514   767  112  202.0  0.982  \\n24      4368  1356  153  478   955  107  156.0  0.983  \\n25      4371  1481  212  504   941  117  162.0  0.981  \\n26      4365  1503  135  568   817  135  175.0  0.979  \\n27      4362  1384  113  465   897   96  173.0  0.985  \\n28      4362  1482  106  544   944  140  186.0  0.977  \\n29      4323  1351  141  543   990  102  156.0  0.984  \\n...      ...   ...  ...  ...   ...  ...    ...    ...  \\n2257    4371  1346   99  509   870  121  152.0  0.981  \\n2258    4398  1350  119  566  1051  133  132.0  0.979  \\n2259    4350  1428  145  446  1012  147  122.0  0.976  \\n2260    4446  1310  122  476   923  142  145.0  0.978  \\n2261    4404  1344  120  519  1059  136  126.0  0.979  \\n2262    4419  1349  127  510   991  110  134.0  0.982  \\n2263    4320  1304  111  584   909  131  128.0  0.979  \\n2264    4404  1296   92  525  1014  124  113.0  0.980  \\n2265    4368  1369  119  521   934  159  144.0  0.975  \\n2266    4324  1353  152  482  1206  126  121.0  0.980  \\n2267    4341  1365  149  557  1138  131  150.0  0.979  \\n2268    4281  1448  156  533  1017  155  127.0  0.975  \\n2269    4302  1505  152  572  1043  160  126.0  0.974  \\n2270    4274  1575  181  579  1011  132  151.0  0.978  \\n2271    4294  1509  190  525  1103  108  139.0  0.982  \\n2272    4359  1475  165  508  1088  139  160.0  0.978  \\n2273    4313  1467  181  463  1028  102  152.0  0.983  \\n2274    4341  1477  191  582  1032   99  172.0  0.984  \\n2275    4374  1456  140  539   997   92  156.0  0.985  \\n2276    4309  1535  193  584   960  131  123.0  0.978  \\n2277    4340  1502  187  580   931  109  153.0  0.982  \\n2278    4302  1496  190  588  1063  123  143.0  0.980  \\n2279    4273  1533  173  629   911  143  155.0  0.977  \\n2280    4305  1469  151  512  1068  127  148.0  0.979  \\n2281    4348  1403  129  477  1049  104  145.0  0.983  \\n2282    4405  1296  129  497  1325   94  134.0  0.985  \\n2283    4337  1367  142  405  1236  107  146.0  0.982  \\n2284    4412  1351  110  352  1288  100  139.0  0.984  \\n2285    4304  1366  145  364  1342   90  125.0  0.985  \\n2286    4379  1272  155  468  1476   73  142.0  0.988  \\n\\n[2287 rows x 29 columns]>'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1440 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We need to create a figure that contains different value of `k` as sub-figures\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "fig.subplots_adjust(left=0,right=1,bottom=0,top=1,hspace=0.05,wspace=0.5)\n",
    "\n",
    "#### complete your code below\n",
    "#### create an empty dictionary `s_score_dict` that we will use to store silhouette scores\n",
    "s_score_dict=[]\n",
    "#### for different `k` values; use different `k` values as keys, and corresponding\n",
    "#### silhouette score as values\n",
    "\n",
    "\n",
    "#### now we create a for-loop go through a range of `k` values in [2, 11]\n",
    "for i in range(2,11):\n",
    "    #### add a sub-figure `ax` to `fig` using `.add_subplot(8,8,i+1,xticks=[],yticks=[])`\n",
    "fig.add_subplot(8,8,i+1,xticks=[],yticks=[])\n",
    "    \n",
    "    # conduct the k-means clustering using `k = i`\n",
    "    km = KMeans(n_clusters=i, random_state=2019)\n",
    "    # any clustering model needs a distance metric, in this case, `distance` is the distance between\n",
    "    # any pair of data points\n",
    "    distances = km.fit_transform(data_features)\n",
    "    # clustering models will generate `labels` - if you want to create the additional feature \n",
    "    # as discussed above, you will use `labels` as its values\n",
    "    labels = km.labels_\n",
    "    # you will then applied the fitted `km` model to `data_faetures`\n",
    "    l= km.fit_predict(data_features)\n",
    "    # Silhouette score is computed between `data_features` and `l`\n",
    "    s_s= metrics.silhouette_score(data_features, l)\n",
    "    #### update the `s_score_dict` using `i` as key and `s_s` as value\n",
    "s_score_dict=\n",
    "    # we will plot the clusters out using scatter plot\n",
    "    plt.scatter(distances[:,0], distances[:,1], c=labels)\n",
    "    #### add 'i clusters' as the title of each sub-figure\n",
    "    \n",
    "    \n",
    "#### show plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, we know that 2-clusters looks the best. Let's double check the silhouette score to make sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_score_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed in the figure, 2-cluster model returns the highest silhouette score. \n",
    "\n",
    "__Rule of thumb__: However, we normally start searching for `k` value at `3`.\n",
    "\n",
    "So we are going to build a k-means model of `k=3`, and then add the `cluster_label` as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### complete your code below\n",
    "#### create a model called `kmeans_model` with `n_clusters = 3` and `random_state = 2019`\n",
    "\n",
    "\n",
    "#### capture `distances` by fit (`fit_transform`) `kmeans_model` to `data_features`\n",
    "\n",
    "\n",
    "#### record labels of clusters in `labels`\n",
    "\n",
    "\n",
    "#### create a scatter plot (plt.scatter()) to plot the clusters\n",
    "\n",
    "\n",
    "#### add title to plot as `3-cluster plot`\n",
    "\n",
    "\n",
    "#### show the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good, correct? Now let's add the `labels` to `data_features` as an additional feature so that we can use it in further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at `labels`\n",
    "print(labels)\n",
    "print(len(labels))\n",
    "print(data_features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### complete your code below\n",
    "#### add `labels` to `data_features`\n",
    "#### add `labels` as a column in `data_features` namely `label`\n",
    "\n",
    "\n",
    "#### double check by looking at the first 5 rows of `data_features`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "We will train linear regression models to predict a continuous target `wins`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### complete your code below\n",
    "#### first we need to create the dataset we will use for the regression model\n",
    "#### `reg_data` = `data_features` + `wins`\n",
    "\n",
    "\n",
    "#### double check by looking at the first 5 rows of `reg_data`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### complete your code below\n",
    "#### investigate descriptive stats using describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the dependencies for building and evaluation a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `LinearRegression` from `sklearn.linear_model`\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Import `mean_absolute_error` from `sklearn.metrics`\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's define the features and target. There are two ways of doing this. Let's try the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### complete your code below\n",
    "#### create a variable `reg_values` which are the values in `reg_data`\n",
    "\n",
    "\n",
    "#### create a variable `X` which contains all columns in `reg_values` besides the last \n",
    "\n",
    "\n",
    "#### create a variable `y` which contains the last column in `reg_values`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an alternative method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### complete your code below\n",
    "#### create a variable `Xa` which contains all values in `data_features`\n",
    "\n",
    "\n",
    "#### create a variable `ya` which contains values in `wins`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to split our data into training (`X_train`, `y_train`) and testing (`X_test`, `y_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### complete your code below\n",
    "#### import `train_test_split` from `sklearn.model_selection`\n",
    "\n",
    "\n",
    "#### split X, y into training and testing, using 75/25 split, and set `random_state = 2019`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Linear Regression model, fit model, and make predictions\n",
    "lr = LinearRegression(normalize=True)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Print `mae`\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print your linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to train an advanced regression model to see if there is any improvement in results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `RidgeCV` from `sklearn.linear_model`\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Create Ridge Linear Regression model, fit model, and make predictions\n",
    "rrm = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0), normalize=True)\n",
    "rrm.fit(X_train, y_train)\n",
    "predictions_rrm = rrm.predict(X_test)\n",
    "\n",
    "# Determine mean absolute error\n",
    "mae_rrm = mean_absolute_error(y_test, predictions_rrm)\n",
    "print(mae_rrm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see how much contribution the `label` feature provides to the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Complete your code below\n",
    "#### create a variable `Xb` without `label`\n",
    "#### you can do it by getting X[:,:-1]\n",
    "\n",
    "#### create your training and testing data using Xb and y\n",
    "#### remember that Xb does not contain 'label', use the same parameters as before\n",
    "#### 75/25 split, and `random_state = 2019`\n",
    "\n",
    "\n",
    "#### Create Linear Regression model, fit model, and make predictions\n",
    "\n",
    "\n",
    "#### calculate the MAE\n",
    "\n",
    "\n",
    "#### Print `mae`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: \n",
    "__Do you observe an improvement or not while excluding `label` in the analysis? In other words, does `label` help with the analysis? Answer in the next block__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Double click and type your answer__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "You will need to create a logistic regression model __on your own__, using `data_features` as features, and `win_bins` as the target.\n",
    "\n",
    "If you have any question, refer to the logistic regression notebook for more help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
